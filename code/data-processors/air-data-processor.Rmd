---
title: "Air Quality Data - PM2.5 Components"
author: "Fatemeh Kazemi"
date: "12-27-2020"
output:
  html_document:
    df_print: paged
---

### This program:
  (1) Downloads and cleans EPA AQS Sites Description Files
  (2) Downloads and cleans PM2.5 Speciation data measured by EPA
      i. https://aqs.epa.gov/aqsweb/airdata/download_files.html#Daily
  (3) Cleans PM2.5 Components data - IMPROVE Network
      i. http://views.cira.colostate.edu/fed/QueryWizard/
  (4) Cleans PM2.5 Components data - CSN Network
      i. http://views.cira.colostate.edu/fed/QueryWizard/
  (5) Combines all available PM2.5 Components data
  (6) Choose a subset of monitors on the basis of data availability
      i. 4days - 9month- 4years
  (7) Smooths data with 4df per year to fill small gaps
      i.Does not smooth over more than 90 days of missing data
      ii. Divides data into coherent pieces instead
  (8) Calculates yearly average of PM2.5 Comonents by site and month
      i. Require at least 350 values to compute yearly average from smoothed data
  (9) Uses cross-walk files between Sites and Zipcodes to assign the measurements
      from Sites to the Zipcodes in their x-mile bufferzone
 (10) Merge it with CMS data (on NEU Cluster) to find the Sites which have
      population living in their x-mile bufferzone
 (11) Runs Factor Analysis (another Rm file) for the subset of Sites with population
      living round them (Sites identified in Step10)
      
### Note:
  1. In data merged with CMS, bufferzone is 12-mile unless specified

```{r load packages}
library(tidyverse)
library(here)
library(lubridate)
library(naniar) # for replacing -999 with NA
```

```{r Call Functions}
source(here('code','data-processors','epa-air-data-download-function.R'))
source(here('code','data-processors','air-data-processor-function.R'))
source(here('code','data-processors','air-data-smooth-mvavg-function.R'))
```

```{r load data}
dirUSSpatial <- 'C:\\Users\\fkazem01\\Box\\Projects\\USA Spatial\\data\\processed\\'
load(paste0(dirUSSpatial,'state-region.RDa'))
load(paste0(dirUSSpatial,'site-zip-closest-bz12.RDa'))
load(paste0(dirUSSpatial,'site-zip.RDa'))
```

```{r - AQS Sites Information}
### AQS Sites - Extract Location setting, land use,
### ... Latitude and Longitude for each site
### https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_site_description_file

temp <- tempfile()
download.file("https://aqs.epa.gov/aqsweb/airdata/aqs_sites.zip", temp)

dt.aqs.sites <- read.csv(unz(temp,"aqs_sites.csv")) %>%
  filter(
    #remove monitors closed before 2000
    year(as.Date(Site.Closed.Date)) >= 2000 | Site.Closed.Date == "",
    !State.Name %in% c("Alaska","Hawaii","Puerto Rico","Virgin Islands",
                       "Guam", "Country Of Mexico","Canada") #remove non-CONUS states
    ) %>%  
  transmute(
    Site.ID = as.factor(sprintf("%02d-%03d-%04d", #unique ID for each Site: ss-ccc-nnnn
                                as.numeric(as.character(State.Code)),
                                County.Code, Site.Number)), 
    State.Code = as.factor(sprintf("%02d", as.numeric(as.character(State.Code)))),
    County.Code = as.factor(sprintf("%03d", County.Code)),
    Latitude,
    Longitude, 
    State = state.abb[match(State.Name,state.name)],
    State = as.factor(ifelse(State.Name == "District Of Columbia","DC", State)),
    Land.Use,
    Location.Setting = as.factor(ifelse(Location.Setting == "URBAN AND CENTER CITY",
                                        "URBAN",
                                        as.character(Location.Setting)))
    ) %>% 
  merge(dt.state.region[, c("State", "Region.IV")]) %>% 
  arrange(Site.ID)

save(dt.aqs.sites, file = here('data', 'processed', 'aqs-sites.RDa'))

dt.aqs.sites %>% 
  group_by(Location.Setting) %>% 
  summarise(n = n())
```

```{r - Parameters' Code and Symbol}
Code.to.Sym <- tribble(
  ~Parameter.Code, ~Parameter.Symbol,
   88103, "AS",
   88111, "CA",
   88114, "CU",
   88115, "CL",
   88126, "FE",
   88128, "PB",
   88132, "MN",
   88136, "NI",
   88140, "MG",
   88154, "SE",
   88164, "V",
   88165, "SI",
   88167, "ZN",
   88180, "K",
   88184, "NA",
   88306, "NO3",
   88403, "SO4",
   88305, "OC.TOT",
   88307, "EC.TOT",
   88320, "OC.TOR",
   88321, "EC.TOR"
   ) %>% 
  mutate(Parameter.Symbol = as.factor(Parameter.Symbol))
```

```{r EPA AQS Data Download}
#Using EPA.AQS.download Function
dt.spec.raw <- EPA.AQS.download (period = "daily", Index = "SPEC",
                        start = 2000, end = 2008, 
                        Code.Sym = Code.to.Sym)
save(dt.spec.raw, file = here('data', 'raw', 'spec-raw.RDa'))
#Monitor.ID: Site.ID-POC
```

```{r EPA AQS Data Exploration}
dt.spec.raw %>% distinct(Monitor.ID) %>% nrow() #659
dt.spec.raw %>% distinct(Site.ID) %>% nrow() #546

# number of sites per year
dt.spec.raw %>% 
  mutate(Year = year(Date)) %>% 
  distinct(Site.ID, Year) %>% 
  group_by(Year) %>% 
  summarise(n = n())

# number of monitors per year
dt.spec.raw %>% 
  mutate(Year = year(Date)) %>% 
  distinct(Monitor.ID, Year) %>% 
  group_by(Year) %>% 
  summarise(n = n())
```

```{r check carbon}
### EC
dt.carbon.ec <- dt.spec.raw %>% 
  select(-Parameter.Code) %>% 
  pivot_wider(names_sort = T,
              names_from = c(Parameter.Symbol),
              values_from = c(Arithmetic.Mean)) %>% 
  arrange(Site.ID, POC, Date) %>%
  select(Site.ID, POC, Monitor.ID, EC.TOR, EC.TOT)

ec.c<- dt.carbon.ec[complete.cases(dt.carbon.ec),] #405 simultaneous TOR & TOT
ec.c %>% group_by(Site.ID) %>% 
  summarise(n = n()) # 11 Sites ~ 33-40 each

summary(lm(EC.TOT ~ EC.TOR, ec.c))
# TOT = 0.730 * TOR + 0.037 Adj.R2 = 0.862

ec.c %>% merge(dt.aqs.sites) %>% 
  distinct(Site.ID, Location.Setting) %>% 
  group_by(Location.Setting) %>% 
  summarise(n = n()) # > Urban and Suburban

### OC
dt.carbon.oc <- dt.spec.raw %>% 
  select(-Parameter.Code) %>% 
  pivot_wider(names_sort = T,
              names_from = c(Parameter.Symbol),
              values_from = c(Arithmetic.Mean)) %>% 
  arrange(Site.ID, POC, Date) %>%
  select(Site.ID, POC, Monitor.ID, OC.TOR, OC.TOT)

oc.c<- dt.carbon.oc[complete.cases(dt.carbon.oc),] #405 simultaneous TOR & TOT
oc.c %>% group_by(Site.ID) %>% 
  summarise(n = n()) # 11 Sites ~ 33-40 each

summary(lm(OC.TOT ~ OC.TOR, oc.c))
# TOT = 0.899 * TOR + 0.167 Adj.R2 = 0.824

oc.c %>% merge(dt.aqs.sites) %>% 
  distinct(Site.ID, Location.Setting) %>% 
  group_by(Location.Setting) %>% 
  summarise(n = n()) # > Urban and Suburban

# For both EC & OC the sites with both methods (TOR & TOT) are in Urban and ...
# ...Sub-Urban area > We use the TOT since most of the Urbans use TOT
```

```{r change spec data to wide format - combine TOR & TOT carbons}
dt.spec.w <- dt.spec.raw %>% 
  select(-Parameter.Code) %>% 
  pivot_wider(names_sort = T,
              names_from = c(Parameter.Symbol),
              values_from = c(Arithmetic.Mean)) %>% 
  # for 405 cases with two methods for EC and OC (TOR & TOT), use TOT 
  mutate(EC = ifelse(!is.na(EC.TOT), EC.TOT, EC.TOR), 
         OC = ifelse(!is.na(OC.TOT), OC.TOT, OC.TOR)) %>% 
  select(-EC.TOR, -EC.TOT, -OC.TOR, -OC.TOT) %>% 
  arrange(Site.ID, POC, Date)

save(dt.spec.w, file = here('data', 'processed', 'spec-wide.RDa'))

dt.spec.w %>% distinct(Monitor.ID) %>% nrow() #659
dt.spec.w %>% distinct(Site.ID) %>% nrow() #546
```

```{r IMPROVE Data cleaning}
# EC 88307 -> TOT
# OC 88320 -> TOR
dt.imprv.raw = data.frame()
for (i in seq(2000,2008,1)){
  dt0 <- read.delim(here('data','raw','improve',paste0('improve_',i,'.txt')), sep = ',')
  dt.imprv.raw <- rbind(dt.imprv.raw, dt0)
}
colnames(dt.imprv.raw) <- colnames(dt.imprv.raw) %>% str_remove("f.Value")

#replace -999 with NA
dt.imprv.raw <- dt.imprv.raw %>% 
  replace_with_na(replace = list("AS"= -999,
                                 "CA"= -999,
                                 "EC"= -999,
                                 "OC"= -999,
                                 "CL"= -999,
                                 "CU"= -999,
                                 "FE"= -999,
                                 "PB"= -999,
                                 "MG"= -999,
                                 "MN"= -999,
                                 "NI"= -999,
                                 "NO3"= -999,
                                 "K"= -999,
                                 "SE"= -999,
                                 "SI"= -999,
                                 "NA"= -999,
                                 "SO4"= -999,
                                 "V"= -999,
                                 "ZN"= -999)) %>% 
  arrange(EPACode, POC, Date)

dt.imprv.raw <- dt.imprv.raw %>% 
  filter(!EPACode == '.') %>% #correct EPACode (Site.ID)
  mutate(EPACode = str_pad(EPACode, 9, pad = "0"))%>% 
  filter(# remove non-CONUS States
    !str_sub(EPACode,1,2) %in% c('02', '15') & str_sub(EPACode,1,2) < '60'
    ) %>% 
  mutate(Date = mdy(Date),
         Site.ID = as.factor(paste(str_sub(EPACode,1,2), str_sub(EPACode,3,5),
                         str_sub(EPACode,6,9), sep = "-")),
         Monitor.ID = as.factor(sprintf("%02d-%03d-%04d-%02d",
                                     as.numeric(str_sub(EPACode,1,2)),
                                     as.numeric(str_sub(EPACode,3,5)),
                                     as.numeric(str_sub(EPACode,6,9)),
                                     POC))) %>% 
  select(Site.ID, POC, Monitor.ID, Date, everything(), -EPACode) %>% 
  arrange(Site.ID, POC, Date)

#remove rows with all components missing
dt.imprv.raw <- dt.imprv.raw[rowSums(is.na(dt.imprv.raw)) != (ncol(dt.imprv.raw)-4), ]

save(dt.imprv.raw, file = here('data', 'raw', 'improve-raw.RDa'))

# find the diff bw spec & improve
monitor.spec <- dt.spec.w %>% distinct(Monitor.ID) #659
monitor.imprv <- dt.imprv.raw %>% distinct(Monitor.ID) #189
monitor.diff.imprv <- setdiff(monitor.imprv$Monitor.ID, monitor.spec$Monitor.ID) %>% 
  as.data.frame() #32
colnames(monitor.diff.imprv) <- 'Monitor.ID'
# improve has 32 Monitor.ID that spec does not have

site.spec <- dt.spec.w %>% distinct(Site.ID) #546
site.imprv <- dt.imprv.raw %>% distinct(Site.ID) #172
site.diff.imprv <- setdiff(site.imprv$Site.ID, site.spec$Site.ID) %>% 
  as.data.frame() #28
colnames(site.diff.imprv) <- 'Site.ID'
# improve has 28 Site.ID that spec does not have
```

```{r CSN Data cleaning}
# EC 88307 -> TOT
# OC 88320 -> TOR
dt.csn.raw = data.frame()
for (i in seq(2000,2008,1)){
  dt0 <- read.delim(here('data','raw','csn',paste0('csn_',i,'.txt')), sep = ',')
  dt.csn.raw <- rbind(dt.csn.raw, dt0)
}
colnames(dt.csn.raw) <- colnames(dt.csn.raw) %>% str_remove("f.Value")

dt.csn.raw <- dt.csn.raw %>% 
  arrange(EPACode, POC, Date)


dt.csn.raw <- dt.csn.raw %>% 
  #correct EPACode (eq. to Site.ID)
  mutate(EPACode = str_pad(EPACode, 9, pad = "0"))%>% 
  filter(# remove non-CONUS States
    !str_sub(EPACode,1,2) %in% c('02', '15') & str_sub(EPACode,1,2) < '60'
    ) %>% 
  mutate(Date = mdy(Date),
         Site.ID = paste(str_sub(EPACode,1,2), str_sub(EPACode,3,5),
                         str_sub(EPACode,6,9), sep = "-"),
         Monitor.ID = as.factor(sprintf("%02d-%03d-%04d-%02d",
                                     as.numeric(str_sub(EPACode,1,2)),
                                     as.numeric(str_sub(EPACode,3,5)),
                                     as.numeric(str_sub(EPACode,6,9)),
                                     POC))) %>% 
  select(Site.ID, POC, Monitor.ID, Date, everything(), -EPACode) %>% 
  arrange(Site.ID, POC, Date)

save(dt.csn.raw, file = here('data', 'raw', 'csn-raw.RDa'))

monitor.spec <- dt.spec.w %>% distinct(Monitor.ID) #659
monitor.csn <- dt.csn.raw %>% distinct(Monitor.ID) #195
monitor.diff.csn <- setdiff(monitor.csn$Monitor.ID, monitor.spec$Monitor.ID) %>% 
  as.data.frame() #0
# no difference between two dataset!
# csn is not adding any new information to spec
```

```{r add new component data to spec data}
# Only IMROVE had new data
# CSN was same as spec
dt.new <- merge(dt.imprv.raw, monitor.diff.imprv)
dt.pmc.w <- rbind(dt.spec.w, dt.new) %>% 
  arrange(Site.ID, POC, Date)

save(dt.pmc.w, file = here('data', 'processed', 'pmc-wide.RDa'))

monitor.pmc <- dt.pmc.w %>% distinct(Monitor.ID) #659 > 691
site.pmc <- dt.pmc.w %>% distinct(Site.ID) #546 > 574
```

```{r label Sites based on the Carbon Method they used - TOR/TOT}
#speciation sites
dt.site.carbon.method <- dt.spec.raw %>% 
  select(-Parameter.Code) %>% 
  pivot_wider(names_from = c(Parameter.Symbol),
              values_from = c(Arithmetic.Mean)) %>% 
  filter(!is.na(EC.TOR) | !is.na(EC.TOT)) %>% 
  mutate(Method = ifelse(!is.na(EC.TOR), "TOR", "TOT")) %>% 
  select(Site.ID, Method) %>% 
  arrange(Site.ID, desc(Method)) %>% #if site has both > keep TOT
  distinct(Site.ID, .keep_all = T)

#improve sites added to speciation sites
dt.site.carbon.method <- site.diff.imprv %>% 
  mutate(Method = "EC.TOT-OC.TOR") %>% 
  add_row(dt.site.carbon.method) %>% 
  arrange(as.character(Site.ID))

save(dt.site.carbon.method, file = here('data', 'processed', 'site-carbon-method.RDa'))

dt.site.carbon.method %>% group_by(Method) %>% 
  summarise(n = n())
# TOT 326
# TOR 167
# EC.TOT-OC.TOR 28
```

```{r Adding Daily PM data to pmc daily data}
# available site-year pmc
site.yr.pmc <- dt.pmc.w %>% 
  mutate(Year = year(Date)) %>% 
  distinct(Site.ID, Year) #3394

# all zipcodes in 12-mile bufferzone of each site each year
dt.pmc.site.all.zip.bz12  <- site.yr.pmc %>% 
  inner_join(dt.site.zip) %>% 
  arrange(Site.ID, Year, Dist) %>% 
  filter(Dist <=12) %>% 
  select(-Zip.Type, -Dist)

save(dt.pmc.site.all.zip.bz12, file = here('data', 'processed', 'pmc-site-all-zip-bz12.RDa'))
write.csv(dt.pmc.site.all.zip.bz12, file = here('data', 'processed', 'pmc-site-all-zip-bz12.csv'))

dt.pmc.site.all.zip.bz12 %>% distinct(Site.ID) %>% nrow() #574 > 558
# 558 out of 574 sites have zipcodes in their 12-mile buffer

# site-close-zip-pmc.csv was uploaded on NEU Cluster to be merged with daily zipcode level
# ... PM data.

# daily PM value for each site based on closest zipcode value
dt.pm.daily <- read.csv(here('data','processed','pm_pmc_daily.csv')) %>% 
  transmute(Site.ID = Site_ID,
            Date = mdy(chardate),
            PM) %>% 
  arrange(Site.ID, Date)

dt.pmc.pm.w <- dt.pmc.w %>% 
  merge(dt.pm.daily)

save(dt.pmc.pm.w, file = here('data', 'processed', 'pmc-pm-wide.RDa'))

dt.pmc.pm.w %>% distinct(Site.ID) %>% nrow() # 574 > 558
dt.pmc.pm.w %>% distinct(Monitor.ID) %>% nrow()#691 > 673

dt.pmc.w[complete.cases(dt.pmc.w),] %>% nrow() #264,927
```

```{r Air quality data Process}
map(dt.pmc.pm.w, ~sum(is.na(.)))

#sites with measurements for all components - spec & improve
dt.pmc.c <- dt.pmc.pm.w[complete.cases(dt.pmc.pm.w),] #243,098
monitor.pmc <- dt.pmc.c %>% distinct(Monitor.ID) # 691 > 673 > 515
site.pmc <- dt.pmc.c %>% distinct(Site.ID) # 574 > 558 > 471

# Find sites with at least 4day, 9month and 4 year
# Using Air.Daily.Process function
dt.pmc.sel <- Air.Daily.Process(dt = dt.pmc.c, d = 4, m = 9, y = 4)

monitor.pmc <- dt.pmc.sel %>% distinct(Monitor.ID) # 691 > 673 > 515 > 336
site.pmc <- dt.pmc.sel %>% distinct(Site.ID) #574 -> 558 > 471 > 322

# POC: Parameter Occurrence Code ...
# ... more than one instrument measured a component at one site
# pick the POC(instrument) with larger number of measurements
site.POC.larger <- dt.pmc.sel %>% 
  group_by(Site.ID, POC) %>% 
  summarise(n = n()) %>% 
  group_by(Site.ID) %>% 
  filter(n == max (n)) %>% 
  select(Site.ID, POC)

dt.pmc.sel <- merge(dt.pmc.sel, site.POC.larger) 
monitor.pmc <- dt.pmc.sel %>% distinct(Monitor.ID) # 691 > 673 > 515 > 336 > 322
site.pmc <- dt.pmc.sel %>% distinct(Site.ID) #574 -> 558 > 471 > 322
dt.pmc.sel$POC <- dt.pmc.sel$Monitor.ID <- NULL
save(dt.pmc.sel, file = here('data', 'processed', 'pmc-selected.RDa'))
```

```{r Smoothing Data & Caluculating 1-yr mvavg for each month}
dt.air <- dt.pmc.sel %>% 
  pivot_longer(cols = AS:PM,
               names_to = "Parameter.Symbol",
               values_to = "Value") %>% 
  arrange(Site.ID, Parameter.Symbol, Date)

site.para <- dt.air %>% 
  distinct(Site.ID, Parameter.Symbol)

#using smoothed.yr function
smooth.out <- mapply(Smoothed.Year, site.para$Site.ID, site.para$Parameter.Symbol)
para.monthly <- data.frame(Site.ID=unlist(smooth.out[1,]),
                           Parameter.Symbol=unlist(smooth.out[2,]),
                           Date=unlist(smooth.out[3,]), 
                           para.1yr=unlist(smooth.out[4,]))

dt.pmc.1yr <- para.monthly %>% 
  mutate(Year = floor(Date -0.01),
         Month = round((Date - Year)*12)) %>% 
  select(-Date) %>% 
  pivot_wider(names_sort = T,
              names_from = c(Parameter.Symbol),
              values_from = c(para.1yr)) %>% 
  arrange(Site.ID, Year, Month)

save(dt.pmc.1yr, file = here('data', 'processed', 'pmc-1yr.RDa'))
write.csv(dt.pmc.1yr, file = here('data', 'processed', 'pmc-1yr.csv'))

dt.site.pmc <- dt.pmc.1yr %>% distinct(Site.ID) %>%  #574 -> 558 > 471 > 318 > 318
  merge(dt.aqs.sites)
save(dt.site.pmc, file = here('data', 'processed', 'site-pmc.RDa'))

dt.site.pmc %>% 
  group_by(Location.Setting) %>% 
  summarise(n = n())
#Location.Setting  n
#                  2			
#RURAL	           177			
#SUBURBAN	         67			
#URBAN	           72	

dt.pmc.1yr %>% 
  distinct(Site.ID, Year) %>% 
  group_by(Site.ID) %>% 
  summarise(n = n()) %>% filter(n > 3) %>% 
  nrow() #318: all  #311: with 4+ years

check <- dt.pmc.1yr %>% 
  group_by(Site.ID) %>% 
  summarise(n = n()) %>% 
  arrange(n)

summary(check$n)
```

```{r Zipcodes in x-mile bufferzone of Monitoring Sites}
# remove components for now to make the dataset smaller
dt.pmc <- dt.pmc.1yr %>% 
  select(Site.ID, Year, Month) %>% 
  mutate(psuedo = 100)

# joining zipcodes to monitoring sites
dt.pmc <- dt.pmc %>% 
  inner_join(dt.site.zip, by = c("Site.ID", "Year")) %>% 
  arrange(Zip.Code, Year, Dist)

# for each zipcode, pick the site with shorter distance to zipcode centroeid
dt.site.dist.shorter <- dt.pmc %>% 
  group_by(Zip.Code, Year) %>% 
  filter(Dist == min (Dist)) %>% 
  arrange(Zip.Code, Year) %>% 
  as.data.frame()

# 6-mile bufferzone around monitoring site
dt.pmc.site.zip.bz6 <- dt.site.dist.shorter %>% 
  filter(Dist <= 6) %>% 
  select(Zip.Code, Year, Month, Site.ID, psuedo)

save(dt.pmc.site.zip.bz6, file = here('data', 'processed', 'pmc-site-zip-bz6.RDa'))
write.csv(dt.pmc.site.zip.bz6, file = here('data', 'processed', 'pmc-site-zip-bz6.csv'))

# Count number of zipcodes and sites
dt.pmc.site.zip.bz6 %>% distinct(Zip.Code) %>% nrow() #4253
dt.pmc.site.zip.bz6 %>% distinct(Site.ID) %>% nrow() #318 > 270

# 12-mile bufferzone around monitoring site
dt.pmc.site.zip.bz12  <- dt.site.dist.shorter %>% 
  filter(Dist <= 12) %>% 
  select(Zip.Code, Year, Month, Site.ID, psuedo)

save(dt.pmc.site.zip.bz12, file = here('data', 'processed', 'pmc-site-zip-bz12.RDa'))
write.csv(dt.pmc.site.zip.bz12, file = here('data', 'processed', 'pmc-site-zip-bz12.csv'))

# Count number of zipcodes and sites
dt.pmc.site.zip.bz12 %>% distinct(Zip.Code) %>% nrow() #8313
dt.pmc.site.zip.bz12 %>% distinct(Site.ID) %>% nrow() #318 > 316

# 24-mile bufferzone around monitoring site
dt.pmc.site.zip.bz24  <- dt.site.dist.shorter %>% 
  filter(Dist <= 24) %>% 
  select(Zip.Code, Year, Month, Site.ID, psuedo)

save(dt.pmc.site.zip.bz24, file = here('data', 'processed', 'pmc-site-zip-bz24.RDa'))
write.csv(dt.pmc.site.zip.bz24, file = here('data', 'processed', 'pmc-site-zip-bz24.csv'))

# Count number of zipcodes and sites
dt.pmc.site.zip.bz24 %>% distinct(Zip.Code) %>% nrow() #15601
dt.pmc.site.zip.bz24 %>% distinct(Site.ID) %>% nrow() #318 > 317

# pmc-site-zip-bz12.csv was uploaded on NEU Cluster to be merged with CMS data
# It has 316 sites with 8313 zipcodes around them
```

```{r Post mergeing with CMS Data}
dt.site.ndi.bz12 <- read.csv(here('data', 'raw', 'site_pmc_ndi_bz12.csv')) %>% 
  rename(Site.ID = Site_ID)

# 325 out of 328 sites have population living in their 12-mile bufferzone
dt.site.ndi.bz12 %>% 
  group_by(Site.ID) %>% 
  summarise(n = n()) %>% filter(n > 0) %>% 
  nrow() # 325 > 317 > 316 > 308: with 4+ years

# TOT/TOR Method in different location.set
dt.site.ndi.bz12 %>% 
  distinct(Site.ID) %>% 
  left_join(dt.site.carbon.method) %>% 
  left_join(dt.aqs.sites) %>% 
  group_by(Location.Setting, Method) %>% 
  summarise(n = n())

# urban and suburban: TOT >> TOR
# rural:              TOR >> TOT
# improve sites only in rural

dt.site.ndi.bz12 %>% 
  distinct(Site.ID) %>% 
  left_join(dt.aqs.sites) %>% 
  group_by(Location.Setting) %>% 
  summarise(n = n())
# urban 72
# suburban 69
# rural 182
```
